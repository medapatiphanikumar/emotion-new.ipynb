{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33e95434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense,Flatten,Dropout\n",
    "from keras.layers import PReLU\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "575197f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data= r\"C:\\Users\\pavani\\OneDrive\\Pictures\\new emotion test\"\n",
    "train_data= r\"C:\\Users\\pavani\\OneDrive\\Pictures\\new emotion train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c950e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_gen= ImageDataGenerator(rescale=1.0/255,\n",
    "                               rotation_range=40,\n",
    "                               width_shift_range=0.2,\n",
    "                               height_shift_range=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5efc0137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "training_data=training_gen.flow_from_directory(train_data,target_size=(48,48),\n",
    "                                              batch_size=32,class_mode='categorical',color_mode ='grayscale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49a44203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "testing_data=testing_gen.flow_from_directory(test_data,target_size=(48,48),\n",
    "                                              batch_size=32,class_mode='categorical',color_mode='grayscale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ff3f923",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e488060",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = keras.models.Sequential()\n",
    "cnn.add(keras.layers.Conv2D(filters=32,padding='same',kernel_size=3,activation='relu',input_shape=[48,48,1]))\n",
    "cnn.add(keras.layers.MaxPool2D(pool_size=2,strides=2))\n",
    "cnn.add(keras.layers.Dropout(0.25))\n",
    "cnn.add(keras.layers.BatchNormalization())\n",
    "cnn.add(keras.layers.Conv2D(filters=32,padding='same',kernel_size=3,activation='relu'))\n",
    "cnn.add(keras.layers.MaxPool2D(pool_size=2,strides=2))\n",
    "cnn.add(keras.layers.Dropout(0.25))\n",
    "cnn.add(keras.layers.BatchNormalization())\n",
    "cnn.add(keras.layers.Conv2D(filters=64,padding='same',kernel_size=3,activation='relu'))\n",
    "cnn.add(keras.layers.MaxPool2D(pool_size=2,strides=2))\n",
    "cnn.add(keras.layers.Dropout(0.25))\n",
    "cnn.add(keras.layers.BatchNormalization())\n",
    "\n",
    "\n",
    "\n",
    "cnn.add(keras.layers.Flatten())\n",
    "cnn.add(keras.layers.Dense(units=128,activation='relu'))\n",
    "cnn.add(keras.layers.Dropout(0.25))\n",
    "cnn.add(keras.layers.BatchNormalization())\n",
    "cnn.add(keras.layers.Dense(units=512,activation='relu'))\n",
    "cnn.add(keras.layers.Dropout(0.25))\n",
    "cnn.add(keras.layers.BatchNormalization())\n",
    "\n",
    "\n",
    "cnn.add(Dense(7,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe0c2eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "898/898 [==============================] - ETA: 0s - loss: 2.0029 - accuracy: 0.2143WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "898/898 [==============================] - 99s 108ms/step - loss: 2.0029 - accuracy: 0.2143 - val_loss: 1.9276 - val_accuracy: 0.2492\n",
      "Epoch 2/30\n",
      "898/898 [==============================] - ETA: 0s - loss: 1.8425 - accuracy: 0.2342WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "898/898 [==============================] - 66s 73ms/step - loss: 1.8425 - accuracy: 0.2342 - val_loss: 1.8721 - val_accuracy: 0.2523\n",
      "Epoch 3/30\n",
      "898/898 [==============================] - ETA: 0s - loss: 1.8168 - accuracy: 0.2410WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "898/898 [==============================] - 60s 66ms/step - loss: 1.8168 - accuracy: 0.2410 - val_loss: 1.8697 - val_accuracy: 0.2200\n",
      "Epoch 4/30\n",
      "897/898 [============================>.] - ETA: 0s - loss: 1.7926 - accuracy: 0.2581WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "898/898 [==============================] - 60s 67ms/step - loss: 1.7927 - accuracy: 0.2581 - val_loss: 1.7438 - val_accuracy: 0.2843\n",
      "Epoch 5/30\n",
      "898/898 [==============================] - ETA: 0s - loss: 1.7794 - accuracy: 0.2659WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "898/898 [==============================] - 60s 67ms/step - loss: 1.7794 - accuracy: 0.2659 - val_loss: 1.6986 - val_accuracy: 0.3050\n",
      "Epoch 6/30\n",
      "898/898 [==============================] - ETA: 0s - loss: 1.7504 - accuracy: 0.2838WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "898/898 [==============================] - 60s 67ms/step - loss: 1.7504 - accuracy: 0.2838 - val_loss: 1.7632 - val_accuracy: 0.3086\n",
      "Epoch 7/30\n",
      "898/898 [==============================] - ETA: 0s - loss: 1.7167 - accuracy: 0.3010WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "898/898 [==============================] - 60s 67ms/step - loss: 1.7167 - accuracy: 0.3010 - val_loss: 1.7081 - val_accuracy: 0.3175\n",
      "Epoch 8/30\n",
      "898/898 [==============================] - ETA: 0s - loss: 1.6871 - accuracy: 0.3180WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "898/898 [==============================] - 65s 73ms/step - loss: 1.6871 - accuracy: 0.3180 - val_loss: 1.5340 - val_accuracy: 0.3863\n",
      "Epoch 9/30\n",
      "898/898 [==============================] - ETA: 0s - loss: 1.6436 - accuracy: 0.3433WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "898/898 [==============================] - 67s 75ms/step - loss: 1.6436 - accuracy: 0.3433 - val_loss: 1.4985 - val_accuracy: 0.4100\n",
      "Epoch 10/30\n",
      "898/898 [==============================] - ETA: 0s - loss: 1.6231 - accuracy: 0.3558WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "898/898 [==============================] - 70s 78ms/step - loss: 1.6231 - accuracy: 0.3558 - val_loss: 1.4378 - val_accuracy: 0.4462\n",
      "Epoch 11/30\n",
      "898/898 [==============================] - ETA: 0s - loss: 1.5868 - accuracy: 0.3759WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "898/898 [==============================] - 64s 71ms/step - loss: 1.5868 - accuracy: 0.3759 - val_loss: 1.4829 - val_accuracy: 0.4305\n",
      "Epoch 12/30\n",
      "898/898 [==============================] - ETA: 0s - loss: 1.5713 - accuracy: 0.3845WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "898/898 [==============================] - 62s 69ms/step - loss: 1.5713 - accuracy: 0.3845 - val_loss: 1.3887 - val_accuracy: 0.4719\n",
      "Epoch 13/30\n",
      "898/898 [==============================] - ETA: 0s - loss: 1.5533 - accuracy: 0.3936WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "898/898 [==============================] - 60s 67ms/step - loss: 1.5533 - accuracy: 0.3936 - val_loss: 1.5040 - val_accuracy: 0.4299\n",
      "Epoch 14/30\n",
      "898/898 [==============================] - ETA: 0s - loss: 1.5383 - accuracy: 0.3978WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "898/898 [==============================] - 62s 68ms/step - loss: 1.5383 - accuracy: 0.3978 - val_loss: 1.5181 - val_accuracy: 0.4206\n",
      "Epoch 15/30\n",
      "898/898 [==============================] - ETA: 0s - loss: 1.5263 - accuracy: 0.4053WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "898/898 [==============================] - 62s 69ms/step - loss: 1.5263 - accuracy: 0.4053 - val_loss: 1.3665 - val_accuracy: 0.4703\n",
      "Epoch 16/30\n",
      "897/898 [============================>.] - ETA: 0s - loss: 1.5169 - accuracy: 0.4094WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "898/898 [==============================] - 545s 607ms/step - loss: 1.5170 - accuracy: 0.4092 - val_loss: 1.3963 - val_accuracy: 0.4550\n",
      "Epoch 17/30\n",
      "898/898 [==============================] - ETA: 0s - loss: 1.5064 - accuracy: 0.4135WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "898/898 [==============================] - 62s 69ms/step - loss: 1.5064 - accuracy: 0.4135 - val_loss: 1.4498 - val_accuracy: 0.4433\n",
      "Epoch 18/30\n",
      "897/898 [============================>.] - ETA: 0s - loss: 1.5255 - accuracy: 0.4072WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "898/898 [==============================] - 60s 67ms/step - loss: 1.5254 - accuracy: 0.4072 - val_loss: 1.2998 - val_accuracy: 0.4955\n",
      "Epoch 19/30\n",
      "898/898 [==============================] - ETA: 0s - loss: 1.4866 - accuracy: 0.4245WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "898/898 [==============================] - 61s 67ms/step - loss: 1.4866 - accuracy: 0.4245 - val_loss: 1.2916 - val_accuracy: 0.5053\n",
      "Epoch 20/30\n",
      "898/898 [==============================] - ETA: 0s - loss: 1.4754 - accuracy: 0.4298WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "898/898 [==============================] - 61s 68ms/step - loss: 1.4754 - accuracy: 0.4298 - val_loss: 1.2585 - val_accuracy: 0.5171\n",
      "Epoch 21/30\n",
      "898/898 [==============================] - ETA: 0s - loss: 1.4732 - accuracy: 0.4270WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "898/898 [==============================] - 312s 347ms/step - loss: 1.4732 - accuracy: 0.4270 - val_loss: 1.2569 - val_accuracy: 0.5163\n",
      "Epoch 22/30\n",
      "898/898 [==============================] - ETA: 0s - loss: 1.4681 - accuracy: 0.4304WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "898/898 [==============================] - 60s 67ms/step - loss: 1.4681 - accuracy: 0.4304 - val_loss: 1.2935 - val_accuracy: 0.4904\n",
      "Epoch 23/30\n",
      "897/898 [============================>.] - ETA: 0s - loss: 1.4574 - accuracy: 0.4370WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "898/898 [==============================] - 59s 66ms/step - loss: 1.4576 - accuracy: 0.4369 - val_loss: 1.3190 - val_accuracy: 0.4858\n",
      "Epoch 24/30\n",
      "898/898 [==============================] - ETA: 0s - loss: 1.4471 - accuracy: 0.4384WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "898/898 [==============================] - 61s 68ms/step - loss: 1.4471 - accuracy: 0.4384 - val_loss: 1.2284 - val_accuracy: 0.5302\n",
      "Epoch 25/30\n",
      "898/898 [==============================] - ETA: 0s - loss: 1.4429 - accuracy: 0.4443WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "898/898 [==============================] - 61s 68ms/step - loss: 1.4429 - accuracy: 0.4443 - val_loss: 1.2174 - val_accuracy: 0.5347\n",
      "Epoch 26/30\n",
      "898/898 [==============================] - ETA: 0s - loss: 1.4406 - accuracy: 0.4456WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "898/898 [==============================] - 61s 67ms/step - loss: 1.4406 - accuracy: 0.4456 - val_loss: 1.2562 - val_accuracy: 0.5089\n",
      "Epoch 27/30\n",
      "898/898 [==============================] - ETA: 0s - loss: 1.4359 - accuracy: 0.4443WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "898/898 [==============================] - 61s 68ms/step - loss: 1.4359 - accuracy: 0.4443 - val_loss: 1.2148 - val_accuracy: 0.5337\n",
      "Epoch 28/30\n",
      "898/898 [==============================] - ETA: 0s - loss: 1.4237 - accuracy: 0.4493WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "898/898 [==============================] - 57s 63ms/step - loss: 1.4237 - accuracy: 0.4493 - val_loss: 1.2104 - val_accuracy: 0.5375\n",
      "Epoch 29/30\n",
      "898/898 [==============================] - ETA: 0s - loss: 1.4227 - accuracy: 0.4505WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "898/898 [==============================] - 58s 65ms/step - loss: 1.4227 - accuracy: 0.4505 - val_loss: 1.3354 - val_accuracy: 0.4857\n",
      "Epoch 30/30\n",
      "898/898 [==============================] - ETA: 0s - loss: 1.4168 - accuracy: 0.4572WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "898/898 [==============================] - 137s 152ms/step - loss: 1.4168 - accuracy: 0.4572 - val_loss: 1.2286 - val_accuracy: 0.5263\n"
     ]
    }
   ],
   "source": [
    "hist=cnn.fit(training_data,epochs=30,verbose=1,validation_data=testing_data,\n",
    "             callbacks=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "860f8d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import load_img\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3dbcf3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image=load_img(r\"C:\\Users\\pavani\\OneDrive\\Pictures\\new emotion test\\neutral\\im14.png\",target_size=(48,48),color_mode='grayscale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1041eb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img=img_to_array(test_image)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1386c3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img1=np.expand_dims(test_img,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "da54520e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "653afab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_gen=ImageDataGenerator(rescale=1.0/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ae60ae5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "images= np.vstack([test_img1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d7d0f975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 131ms/step\n"
     ]
    }
   ],
   "source": [
    "out= np.argmax(cnn.predict(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7535d067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c7ceb3f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'angry': 0,\n",
       " 'disgusted': 1,\n",
       " 'fearful': 2,\n",
       " 'happy': 3,\n",
       " 'neutral': 4,\n",
       " 'sad': 5,\n",
       " 'surprised': 6}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7f36d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5b60f1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('./emotionmodel.h5',monitor='val_acc',verbose=1,save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8509c648",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.models.save_model(cnn,'./emotionmodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0a0209a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c60d714c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "import cv2\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab591966",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detector = cv2.CascadeClassifier(r\"C:\\Users\\pavani\\Downloads\\haarcascade_frontalface_default.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5eea534",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_dict = {'angry': 0,\n",
    " 'disgusted': 1,\n",
    " 'fearful': 2,\n",
    " 'happy': 3,\n",
    " 'neutral': 4,\n",
    " 'sad': 5,\n",
    " 'surprised': 6}\n",
    "\n",
    "cap = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9846562",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    ret,test_img = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "    gray_img = cv2.cvtColor(test_img,cv2.COLOR_BGR2GRAY)    \n",
    "    \n",
    "    faces = face_detector.detectMultiScale(gray_img,1.32,5)\n",
    "    \n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(test_img,(x,y),(x+w,y+h),(255,0,0),thickness=7)\n",
    "        roi_gray = gray_img[y:y+w,x:x+h]\n",
    "        roi_gray = cv2.resize(roi_gray,(48,48))\n",
    "        #roi_gray1 = roi_gray.reshape(1,48,48,1)\n",
    "        img_pixels = image.img_to_array(roi_gray)\n",
    "        img_pixels = np.expand_dims(img_pixels,axis=0)\n",
    "        img_pixels /= 255\n",
    "        \n",
    "        predictions = emotion_model.predict(img_pixels)\n",
    "        \n",
    "        max_index = np.argmax(predictions[0])\n",
    "        \n",
    "        predicted_emotion = emotion_dict[max_index]\n",
    "        \n",
    "        cv2.putText(test_img,predicted_emotion,(int(x),int(y)),cv2.FONT_HERSHEY_SIMPLEX,1)\n",
    "        \n",
    "    resized_img = cv2.resize(test_img,(1000,700))\n",
    "    cv2.imshow('emotion analysis',resized_img)\n",
    "    \n",
    "    if cv2.waitKey(10) == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()        \n",
    "cv2.destroyAllWindows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f366d440",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
